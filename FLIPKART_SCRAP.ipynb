{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3b3453-776a-49c2-ad30-68bd207aa334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84bad0cc-4c43-479c-a88e-e6260fa04938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 186\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "No 'Next' button found. Quitting at page 11.\n",
      "Scraping completed\n",
      "Last page scraped: 11\n",
      "Total reviews processed: 100\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/vivo-t3-pro-5g-emerald-green-256-gb/product-reviews/itmcf52c1fcffbf3?pid=MOBH3XHRUQMKBEZX&lid=LSTMOBH3XHRUQMKBEZXUBLAII&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfv1 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfv1.to_csv('v1_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2583b76-9425-443b-b2a4-56a4ed59446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 63\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "No 'Next' button found. Quitting at page 11.\n",
      "Scraping completed\n",
      "Last page scraped: 11\n",
      "Total reviews processed: 100\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/vivo-v40-5g-lotus-purple-128-gb/product-reviews/itm444f2b97f5db4?pid=MOBH33ZXNTFBZGQB&lid=LSTMOBH33ZXNTFBZGQB7C8BVT&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfv2 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfv2.to_csv('v2_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00b95817-9c34-4264-b913-522274a72015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 21\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "No 'Next' button found. Quitting at page 11.\n",
      "Scraping completed\n",
      "Last page scraped: 11\n",
      "Total reviews processed: 100\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/vivo-v23-pro-5g-stardust-black-256-gb/product-reviews/itmf5550a0e86c82?pid=MOBGA25PUSPF5JBE&lid=LSTMOBGA25PUSPF5JBEUSH5WL&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfv3 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfv3.to_csv('v3_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea35573e-7606-4532-9870-4dd1f74d21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1ebd92d-3031-46f5-ba7e-e95b45994fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 6\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "No reviews found on page 7. Skipping...\n",
      "No 'Next' button found. Quitting at page 7.\n",
      "Scraping completed\n",
      "Last page scraped: 7\n",
      "Total reviews processed: 56\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/samsung-galaxy-a55-5g-awesome-iceblue-128-gb/product-reviews/itm0bb662185bcc4?pid=MOBGYT2HX4A4QAWW&lid=LSTMOBGYT2HX4A4QAWWILKKCV&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfs1 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfs1.to_csv('s1_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc93e7df-2841-45c7-9c30-9a44dfae8bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 36\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "No 'Next' button found. Quitting at page 11.\n",
      "Scraping completed\n",
      "Last page scraped: 11\n",
      "Total reviews processed: 100\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/samsung-galaxy-a54-5g-awesome-violet-128-gb/product-reviews/itm4bbcb0b5e1b2d?pid=MOBGNE4SRZG7HKW4&lid=LSTMOBGNE4SRZG7HKW4YEAWO7&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfs2 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfs2.to_csv('s2_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0926637-0188-49b6-bf3a-31346ccaf873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 334\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "No reviews found on page 20. Skipping...\n",
      "Scraping page 21\n",
      "No reviews found on page 21. Skipping...\n",
      "No 'Next' button found. Quitting at page 21.\n",
      "Scraping completed\n",
      "Last page scraped: 21\n",
      "Total reviews processed: 120\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/samsung-galaxy-s23-fe-purple-256-gb/product-reviews/itm8f6a49271bf21?pid=MOBGVTA2VGHCJFGG&lid=LSTMOBGVTA2VGHCJFGG1TC2LI&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfs3 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfs3.to_csv('s3_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a408ca8-875e-4353-b422-228027922fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#google pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bd04b3-1aef-4a00-8a68-556678f796ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 16\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "No 'Next' button found. Quitting at page 11.\n",
      "Scraping completed\n",
      "Last page scraped: 11\n",
      "Total reviews processed: 100\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/google-pixel-8a-bay-128-gb/product-reviews/itm6d2e15988f2c4?pid=MOBGYQ2MQYHU59Y2&lid=LSTMOBGYQ2MQYHU59Y2IR0ZZ0&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfg1 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfg1.to_csv('g1_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6894210-8fa3-4b27-b738-ac72c00deb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 193\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "No reviews found on page 21. Skipping...\n",
      "No 'Next' button found. Quitting at page 21.\n",
      "Scraping completed\n",
      "Last page scraped: 21\n",
      "Total reviews processed: 128\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/google-pixel-7-lemongrass-128-gb/product-reviews/itm45d75002be0e7?pid=MOBGHW44ZSN5EPGU&lid=LSTMOBGHW44ZSN5EPGURSVVHV&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfg2 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfg2.to_csv('g2_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e667020c-f927-47b0-8c84-3e4829a7ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oneplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0da23bd-5f91-449a-80e8-d00596329f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 41\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "Scraping page 21\n",
      "No reviews found on page 21. Skipping...\n",
      "No 'Next' button found. Quitting at page 21.\n",
      "Scraping completed\n",
      "Last page scraped: 21\n",
      "Total reviews processed: 143\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/oneplus-12r-iron-gray-128-gb/product-reviews/itm347349f7db2f2?pid=MOBGXGT7YP8GPTGU&lid=LSTMOBGXGT7YP8GPTGUSDHNTF&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfo1 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfo1.to_csv('o1_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc1d0e9-6bba-4f73-a496-aa8bc63af83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 117\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "No reviews found on page 11. Skipping...\n",
      "No 'Next' button found. Quitting at page 11.\n",
      "Scraping completed\n",
      "Last page scraped: 11\n",
      "Total reviews processed: 100\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/oneplus-nord-3-5g-tempest-gray-128-gb/product-reviews/itm5fc87afce35dc?pid=MOBGRK2VXCKBADB5&lid=LSTMOBGRK2VXCKBADB5XYKXX3&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfo2 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfo2.to_csv('o2_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dac785c-9e67-4914-9abf-1e7a5b611ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa7bd05d-26c6-48ca-9005-d404757f3364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 53\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "No reviews found on page 12. Skipping...\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "No reviews found on page 20. Skipping...\n",
      "Scraping page 21\n",
      "No reviews found on page 21. Skipping...\n",
      "No 'Next' button found. Quitting at page 21.\n",
      "Scraping completed\n",
      "Last page scraped: 21\n",
      "Total reviews processed: 132\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/redmi-note-13-pro-5g-arctic-white-256-gb/product-reviews/itm409ded54ee4f1?pid=MOBGWFHFC4XGEEAA&lid=LSTMOBGWFHFC4XGEEAANY6BLQ&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfr1 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfr1.to_csv('r1_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c78d8139-b5be-4fa4-b4ee-235356c95bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages to scrape: 228\n",
      "Scraping page 1\n",
      "Scraping page 2\n",
      "Scraping page 3\n",
      "Scraping page 4\n",
      "Scraping page 5\n",
      "Scraping page 6\n",
      "Scraping page 7\n",
      "Scraping page 8\n",
      "Scraping page 9\n",
      "Scraping page 10\n",
      "Scraping page 11\n",
      "Scraping page 12\n",
      "Scraping page 13\n",
      "Scraping page 14\n",
      "Scraping page 15\n",
      "Scraping page 16\n",
      "Scraping page 17\n",
      "Scraping page 18\n",
      "Scraping page 19\n",
      "Scraping page 20\n",
      "No reviews found on page 20. Skipping...\n",
      "Scraping page 21\n",
      "No reviews found on page 21. Skipping...\n",
      "No 'Next' button found. Quitting at page 21.\n",
      "Scraping completed\n",
      "Last page scraped: 21\n",
      "Total reviews processed: 129\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.maximize_window()\n",
    "\n",
    "# Page URL for taking Data\n",
    "base_url = \"https://www.flipkart.com/redmi-note-13-pro-5g-coral-purple-256-gb/product-reviews/itmc22127928b44d?pid=MOBGWFHFAFBPDZHG&lid=LSTMOBGWFHFAFBPDZHGGQOPOF&marketplace=FLIPKART\"\n",
    "driver.get(base_url)\n",
    "time.sleep(2)\n",
    "product_id = base_url.split(\"pid=\")[-1].split(\"&\")[0]\n",
    "\n",
    "# Function to get the total number of pages\n",
    "def get_total_pages():\n",
    "    try:\n",
    "        # Locate the element with the total pages\n",
    "        page_info = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div/div[2]/div[13]/div/div/span[1]').text\n",
    "        total_pages = int(page_info.split(\"of\")[-1].strip())  # Extract the total page number\n",
    "        return total_pages\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting total pages: {e}\")\n",
    "        return 1  # Default to 1 if unable to extract total pages\n",
    "\n",
    "# Function to scrape reviews, product name, and price\n",
    "def scrape_reviews():\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        # Extract product name\n",
    "        product_name_element = driver.find_element(By.XPATH, '//*[@id=\"container\"]/div/div[3]/div/div[1]/div[1]/div[1]/div/div/div[1]/a')\n",
    "        product_name = product_name_element.get_attribute(\"title\")\n",
    "\n",
    "        # Extract product price\n",
    "        product_price_element = driver.find_element(By.CLASS_NAME, \"Nx9bqj\")\n",
    "        product_price = product_price_element.text\n",
    "\n",
    "        # Extract reviews, ratings, and one-word reviews\n",
    "        review_elements = driver.find_elements(By.CLASS_NAME, \"ZmyHeo\")\n",
    "        rating_elements = driver.find_elements(By.CLASS_NAME, \"XQDdHH.Ga3i8K\")\n",
    "        one_word_review_elements = driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "        \n",
    "        # Ensure all lists have the same length to avoid mismatches\n",
    "        for i in range(min(len(review_elements), len(rating_elements), len(one_word_review_elements))):\n",
    "            review = review_elements[i].text\n",
    "            rating = rating_elements[i].text\n",
    "            one_word_review = one_word_review_elements[i].text\n",
    "            reviews_data.append([product_id, product_name, product_price, review, rating, one_word_review])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping reviews: {e}\")\n",
    "    \n",
    "    return reviews_data\n",
    "\n",
    "# Function to check for the \"Next\" button if not exit the Page\n",
    "def has_next_page():\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]')  # Flipkart's \"Next\" button\n",
    "        return next_button.is_enabled()\n",
    "    except Exception:\n",
    "        return False  # Return False if the button is not found or is disabled\n",
    "\n",
    "\n",
    "all_reviews_data = []\n",
    "page = 1\n",
    "total_pages = get_total_pages() \n",
    "print(f\"Total pages to scrape: {total_pages}\")\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page}\")\n",
    "    url = f\"{base_url}&page={page}\"\n",
    "    # Load the page with the current page number\n",
    "    driver.get(url)\n",
    "    time.sleep(1)  # Reduced delay for faster scraping\n",
    "\n",
    "    # Scrape reviews from the current page\n",
    "    page_reviews = scrape_reviews()\n",
    "    \n",
    "    # If no reviews are found, skip to the next page\n",
    "    if not page_reviews:\n",
    "        print(f\"No reviews found on page {page}. Skipping...\")\n",
    "    else:\n",
    "        all_reviews_data.extend(page_reviews)\n",
    "\n",
    "    # Check if there's a \"Next\" button, and stop if it's not present\n",
    "    if not has_next_page():\n",
    "        print(f\"No 'Next' button found. Quitting at page {page}.\")\n",
    "        break\n",
    "\n",
    "    page += 1  # Move to the next page\n",
    "\n",
    "# Saving scraped data to CSV using Pandas\n",
    "dfr2 = pd.DataFrame(all_reviews_data, columns=['Product ID', 'Product Name', 'Price', 'Review', 'Rating', 'One-word Review'])\n",
    "dfr2.to_csv('r2_review.csv', index=False)\n",
    "\n",
    "print(\"Scraping completed\")\n",
    "print(f\"Last page scraped: {page}\")\n",
    "print(f\"Total reviews processed: {len(all_reviews_data)}\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11068d70-9e5a-4ec3-832f-18d8d609bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ff040-790d-4826-820c-a8a75dd00596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
